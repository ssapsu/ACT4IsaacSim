{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d208da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy HDF5 file \"dummy_dataset.hdf5\" created with:\n",
      " - 10 images per topic\n",
      " - 10 semantic labels\n",
      " - 10 cmd_vel entries\n",
      " - 10 joint_states entries\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "filename = 'dummy_dataset.hdf5'\n",
    "num_images = 10\n",
    "H, W = 480, 640\n",
    "num_labels = 10\n",
    "num_cmd = 10\n",
    "num_joints = 6\n",
    "num_joint_states = 10\n",
    "\n",
    "# Create dummy HDF5 file\n",
    "with h5py.File(filename, 'w') as f:\n",
    "    # Images group\n",
    "    imgs = f.create_group('images')\n",
    "    imgs.create_dataset(\n",
    "        'color',\n",
    "        data=np.random.randint(0, 256, size=(num_images, H, W, 3), dtype='uint8'),\n",
    "        chunks=(1, H, W, 3)\n",
    "    )\n",
    "    imgs.create_dataset(\n",
    "        'depth',\n",
    "        data=np.random.rand(num_images, H, W).astype('float32'),\n",
    "        chunks=(1, H, W)\n",
    "    )\n",
    "    imgs.create_dataset(\n",
    "        'left_color',\n",
    "        data=np.random.randint(0, 256, size=(num_images, H, W, 3), dtype='uint8'),\n",
    "        chunks=(1, H, W, 3)\n",
    "    )\n",
    "    imgs.create_dataset(\n",
    "        'semantic_segmentation',\n",
    "        data=np.random.randint(0, 5, size=(num_images, H, W), dtype='uint8'),\n",
    "        chunks=(1, H, W)\n",
    "    )\n",
    "\n",
    "    # Semantic labels\n",
    "    dt = h5py.string_dtype(encoding='utf-8')\n",
    "    labels_grp = f.create_group('semantic_labels')\n",
    "    labels_grp.create_dataset(\n",
    "        'value',\n",
    "        data=np.array([f'label_{i}' for i in range(num_labels)], dtype=object),\n",
    "        dtype=dt,\n",
    "        chunks=(1,)\n",
    "    )\n",
    "\n",
    "    # cmd_vel group\n",
    "    cmd = f.create_group('cmd_vel')\n",
    "    cmd.create_dataset('stamp', data=np.linspace(0, 1, num_cmd), dtype='float64', chunks=(1,))\n",
    "    cmd.create_dataset('linear', data=np.random.randn(num_cmd, 3).astype('float32'), chunks=(1, 3))\n",
    "    cmd.create_dataset('angular', data=np.random.randn(num_cmd, 3).astype('float32'), chunks=(1, 3))\n",
    "\n",
    "    # joint_states group\n",
    "    js = f.create_group('joint_states')\n",
    "    js.create_dataset('stamp', data=np.linspace(0, 1, num_joint_states), dtype='float64', chunks=(1,))\n",
    "    js.create_dataset('position', data=np.random.randn(num_joint_states, num_joints).astype('float32'), chunks=(1, num_joints))\n",
    "    js.create_dataset('velocity', data=np.random.randn(num_joint_states, num_joints).astype('float32'), chunks=(1, num_joints))\n",
    "    js.create_dataset('effort', data=np.random.randn(num_joint_states, num_joints).astype('float32'), chunks=(1, num_joints))\n",
    "\n",
    "print(f'Dummy HDF5 file \"{filename}\" created with:')\n",
    "print(f' - {num_images} images per topic')\n",
    "print(f' - {num_labels} semantic labels')\n",
    "print(f' - {num_cmd} cmd_vel entries')\n",
    "print(f' - {num_joint_states} joint_states entries')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aloha",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
